[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "1 Assignmnet\n\n# Load the file\nrandom_vars &lt;- readRDS(\"Causal_Data_Science_Data/random_vars.rds\")\n# Create a Data Frame\ndf1=data.frame(random_vars)\n\n\nFind the values of mean, variance and standard deviation\n\n\n# 1.Mean \nmean(df1$age)\n\n#&gt; [1] 33.471\n\nmean(df1$income)\n\n#&gt; [1] 3510.731\n\n\n\n# 2. Variance\nvar(df1$age)\n\n#&gt; [1] 340.6078\n\nvar(df1$income)\n\n#&gt; [1] 8625646\n\n\n\n# 3. Standard deviation\nsd(df1$age)\n\n#&gt; [1] 18.45556\n\nsd(df1$income)\n\n#&gt; [1] 2936.945\n\n\n\nThe calculated Standard deviations are on vastly different scales. In this case, the large difference in values suggests that the Data Sets corresponding to these standard deviations have very different degrees of variability. Due to this reason it does not make sense to compare them.\nCalculate Covariance and Correlation\n\n\n# Covariance\ncov(df1)\n\n#&gt;               age     income\n#&gt; age      340.6078   29700.15\n#&gt; income 29700.1468 8625645.84\n\n\n\n# Correlation\ncor(df1)\n\n#&gt;              age    income\n#&gt; age    1.0000000 0.5479432\n#&gt; income 0.5479432 1.0000000\n\n\n\nCorrelation is generally easier to interpret and compare than covariance especially when dealing with variables on different scales. It provides a standardized measure of the strength and direction of the linear relationship between two variables.\nCompute the Conditional Expected value\n\n\n# 1\nage_18 &lt;- subset(df1, age&lt;=18, select = c(age,income))\nmean(age_18$income)\n\n#&gt; [1] 389.6074\n\n\n\n# 2\nage_65 &lt;- subset(df1, age&gt;=65, select = c(age,income))\nmean(age_65$income)\n\n#&gt; [1] 1777.237\n\n\n\n# 3\nage_18_65 &lt;- subset(df1, age&gt;=18 & age&lt;65, select = c(age,income))\nmean(age_18_65$income)\n\n#&gt; [1] 4685.734"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "1 Assignment\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n\n# Generate random data for two variables\nn &lt;- 50\nx &lt;- rnorm(n)\ny &lt;- 0.8*x + rnorm(n)\n\n\n# Create a data frame\ndata &lt;- data.frame(x = x, y = y)\n\n\n# Create a scatter plot with a regression line\nggplot(data, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Spurious Correlation\",\n       x = \"X Variable\",\n       y = \"Y Variable\") +\n  theme_minimal()\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dplyr)\nlibrary(ggdag)\nlibrary(dagitty)\nlibrary(ggrepel)\n\n\n# Load the file\nrand_enc &lt;- readRDS(\"Causal_Data_Science_Data/rand_enc.rds\")\n# Create a Data Frame\ndf=data.frame(rand_enc)\n\n\nPlot DAG\n\n\nApp_usuage &lt;- dagify(\n  Y ~ D,\n  D ~ Z,\n  coords = list(x = c(D = 0.5, Y = 2, Z = -1),\n                y = c(D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"used_ftr\",\n             \"Y\" = \"timeSpent\",\n             \"Z\" = \"rand_enc\")\n)\n\nggdag(App_usuage, text = TRUE) +\n  geom_dag_point(color = \"blue\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\nFrom my understanding of these relationships from DAG, time spent would be the outcome which depends on the used new feature (used_ftr) would be the treatment variable and randomized encouragement trial (rand_enc) would be instrumental variable.\n\nCompute the naive, biased estimate.\n\n\nmodel_biased &lt;- lm(time_spent ~ used_ftr, data = df)\nsummary(model_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodel_naive &lt;- lm(rand_enc ~ used_ftr, data = df)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = rand_enc ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.6299 -0.4220  0.3701  0.3701  0.5780 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 0.421959   0.006361   66.33   &lt;2e-16 ***\n#&gt; used_ftr    0.207943   0.009959   20.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4894 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.04178,    Adjusted R-squared:  0.04169 \n#&gt; F-statistic:   436 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\ncor(df$rand_enc, df$used_ftr)\n\n#&gt; [1] 0.204411\n\n\nThe positive correlation indicates that users who received the encouragement were more likely to use the new feature.\n\nCompute the IV estimate using 2SLS and compare it to the naive estimate\n\n\nlibrary(estimatr)\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = df)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = df)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\nOn Computing the IV estimate using 2SLS and comparing it to the naive estimate, the naive estimate for used_ftr (0.207943) seems to be biased downward."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "# Load packages\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggrepel)\n\n\n# Load the file\nmembership &lt;- readRDS(\"Causal_Data_Science_Data/membership.rds\")\n# Create a Data Frame\ndf=data.frame(membership)\n\n\nDraw DAG to understand the relations between variables\n\n\n# Confounding variables are age, sex, prev_avg_purch\npurchase_dag &lt;- dagify(\n  card  ~ age + sex + prev_avg_purch, sex ~ age , prev_avg_purch ~ sex, avg_purch ~ card ,\n  coords = list(x = c(age = 1,sex = 2, prev_avg_purch = 3, card = 2, avg_purch = 2),\n                      y = c(age = 1,sex = 1, prev_avg_purch = 1, card = 3, avg_purch = 4)  )\n)\n\nggdag(purchase_dag, text = TRUE) +\n  geom_dag_point(color = \"blue\") +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges(edge_color = \"black\")\n\n\n\n\n\n\n\n\nSales are described by average purchases and they depend on the membership cards directly, but as a back door path, they also depend on age, sex, and previous average purchase. Hence, the arrows are indicated accordingly.\n\nNaive estimate of the average treatment effect.\n\n\nmodel_naive &lt;- lm(avg_purch ~ card, data = df)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Subclassification estimator (subclasses: Z = 0 and Z = 1)\n# E(Z, D)\nE_00 &lt;- mean(df[(df$sex==F & df$card==F), ]$avg_purch) \nE_10 &lt;- mean(df[(df$sex==T & df$card==F), ]$avg_purch) \nE_01 &lt;- mean(df[(df$sex==F & df$card==T), ]$avg_purch) \nE_11 &lt;- mean(df[(df$sex==T & df$card==T), ]$avg_purch) \n\n# Weighted by K (proportion of female/male)\nK &lt;- mean(df$sex)\n\nK*(E_11-E_10) + (1-K)*(E_01 - E_00)\n\n#&gt; [1] 25.22093\n\n\n\nUsing the following matching methods to obtain more precise estimates:\n\n3.1 Coarsened exact matching\n\n# Load 'MatchIt' library\nlibrary(MatchIt)\n\n# Without specifying coarsening\n# (1) Matching\ncem &lt;- matchit(card ~ age + pre_avg_purch,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = df, method = \"cem\", \n#&gt;     estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.2869       40.2645          0.0017     1.0001    0.0016\n#&gt; pre_avg_purch       70.5402       70.1875          0.0135     0.9910    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0067          0.1224\n#&gt; pre_avg_purch   0.0133          0.1557\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5527.11 3928.04\n#&gt; Matched       5752.   4199.  \n#&gt; Unmatched       16.     33.  \n#&gt; Discarded        0.      0.\n\n# Use matched data\ndf_cem &lt;- match.data(cem)\n\n# (2) Estimation\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -155.31  -20.74   -0.17   20.25  146.91 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.0474     0.3996  175.30   &lt;2e-16 ***\n#&gt; card         15.2687     0.6151   24.82   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.31 on 9949 degrees of freedom\n#&gt; Multiple R-squared:  0.05831,    Adjusted R-squared:  0.05822 \n#&gt; F-statistic: 616.1 on 1 and 9949 DF,  p-value: &lt; 2.2e-16\n\n\n3.2 Nearest neighbour matching\n\n# (1) Matching\n\nnn &lt;- matchit(card ~ age + pre_avg_purch,\n              data = df,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              )\n\n# Covariate Balance\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch, data = df, method = \"nearest\", \n#&gt;     distance = \"mahalanobis\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.2938          0.0531     1.1056    0.0111\n#&gt; pre_avg_purch       76.3938       74.0433          0.0894     1.2062    0.0200\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0284          0.0772\n#&gt; pre_avg_purch   0.0633          0.1112\n#&gt; \n#&gt; Sample Sizes:\n#&gt;           Control Treated\n#&gt; All          5768    4232\n#&gt; Matched      4232    4232\n#&gt; Unmatched    1536       0\n#&gt; Discarded       0       0\n\n# Use matched data\ndf_nn &lt;- match.data(nn)\n\n# (2) Estimation\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -19.954    0.074   19.602  112.234 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  73.8717     0.4509  163.83   &lt;2e-16 ***\n#&gt; card         17.2875     0.6377   27.11   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 29.33 on 8462 degrees of freedom\n#&gt; Multiple R-squared:  0.07991,    Adjusted R-squared:  0.0798 \n#&gt; F-statistic: 734.9 on 1 and 8462 DF,  p-value: &lt; 2.2e-16\n\n\n3.3 Inverse probability weighting\n\n# (1) Propensity scores\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4121735  0.0723729 -19.512   &lt;2e-16 ***\n#&gt; age            0.0011734  0.0017758   0.661    0.509    \n#&gt; pre_avg_purch  0.0148181  0.0009263  15.996   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13250  on 9997  degrees of freedom\n#&gt; AIC: 13256\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n# Add propensities to table\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n\n# Extend data by IPW scores\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\n# Look at data with IPW scores\ndf_ipw %&gt;% \n  select(card, age, pre_avg_purch , propensity, ipw)\n\n\n\n  \n\n\n\n\n# (2) Estimation\nmodel_ipw &lt;- lm(avg_purch ~ card,\n                data = df_ipw, \n                weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -372.55  -28.71   -0.14   30.70  428.34 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 76.32512    0.46045  165.76   &lt;2e-16 ***\n#&gt; card         0.01967    0.65007    0.03    0.976    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 45.97 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  9.161e-08,  Adjusted R-squared:  -9.993e-05 \n#&gt; F-statistic: 0.0009159 on 1 and 9998 DF,  p-value: 0.9759\n\n\n\n# Plot histogram of estimated propensities\nggplot(df_aug, aes(x = propensity)) +\n  geom_histogram(alpha = .8, color = \"white\")\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n# Looking for observations with highest weights\ndf_ipw %&gt;% \n  select(card, age, pre_avg_purch, propensity, ipw) %&gt;% \n  arrange(desc(ipw))\n\n\n\n  \n\n\n\n\n# Run with high weights excluded\nmodel_ipw_trim &lt;- lm(avg_purch ~ card,\n                data = df_ipw %&gt;% filter(propensity %&gt;% between(0.15, 0.85)),\n                weights = ipw)\nsummary(model_ipw_trim)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw %&gt;% filter(propensity %&gt;% \n#&gt;     between(0.15, 0.85)), weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -142.165  -28.049   -1.863   27.190  193.352 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  80.0714     0.4107 194.978   &lt;2e-16 ***\n#&gt; card         -0.1562     0.5811  -0.269    0.788    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 39.41 on 9192 degrees of freedom\n#&gt; Multiple R-squared:  7.863e-06,  Adjusted R-squared:  -0.0001009 \n#&gt; F-statistic: 0.07228 on 1 and 9192 DF,  p-value: 0.7881\n\n\n\n# Summary of naive and matching methods\nmodelsummary::modelsummary(list(\"Naive\" = model_naive,\n                                \"CEM1\"  = model_cem,\n                                \"NN\"    = model_nn,\n                                \"IPW1\"  = model_ipw,\n                                \"IPW2\"  = model_ipw_trim))\n\n\n\n\n\nNaive\nCEM1\nNN\n IPW1\n IPW2\n\n\n\n\n(Intercept)\n65.940\n70.047\n73.872\n76.325\n80.071\n\n\n\n(0.397)\n(0.400)\n(0.451)\n(0.460)\n(0.411)\n\n\ncard\n25.220\n15.269\n17.287\n0.020\n−0.156\n\n\n\n(0.610)\n(0.615)\n(0.638)\n(0.650)\n(0.581)\n\n\nNum.Obs.\n10000\n9951\n8464\n10000\n9194\n\n\nR2\n0.146\n0.058\n0.080\n0.000\n0.000\n\n\nR2 Adj.\n0.146\n0.058\n0.080\n0.000\n0.000\n\n\nAIC\n96483.2\n96352.4\n81218.9\n98949.5\n87941.3\n\n\nBIC\n96504.8\n96374.0\n81240.1\n98971.2\n87962.7\n\n\nLog.Lik.\n−48238.590\n−48173.187\n−40606.465\n−49471.774\n−43967.674\n\n\nRMSE\n30.11\n30.27\n29.33\n32.59\n27.82"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "1 Assignment\n\n# Load necessary Libraries\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggrepel)\n\n\nDAG for Parking Spots\n\n\nParking_Spots &lt;- dagify(\n  Y ~ X,\n  X ~ Z,\n  Y ~ Z,\n  coords = list(x = c(Y = 3, Z = 2, X = 1),\n                y = c(Y = 0, Z = 1, X = 0)),\n  labels = c(\"X\" = \"ParkingSpots\",\n             \"Y\" = \"Sales\",\n             \"Z\" = \"Location\")\n)\n\nggdag(Parking_Spots, text = TRUE) +\n  geom_dag_point(color = \"blue\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\nRegression\n\n\n# Load the file\ncustomer_sat &lt;- readRDS(\"Causal_Data_Science_Data/customer_sat.rds\")\n# Create a Data Frame\ndf=data.frame(customer_sat)\n\n2.1 Regress satisfaction on follow_ups\n\n#lm_2_1 is the regresssion of satisfaction on follow_ups\nlm_2_1 &lt;- lm(satisfaction ~ follow_ups , data = df)\nsummary(lm_2_1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\n2.2 Regress satisfaction on follow_ups and account for subscription\n\n#lm_2_2 is the regresssion of satisfaction on follow_ups and account for subscription\nlm_2_2 &lt;- lm(satisfaction ~ follow_ups + subscription , data = df)\nsummary(lm_2_2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\n\nComparing the coefficients:\n\nIn the first regression with satisfaction on follow_ups are negatively correlated as increase in satisfaction with the product and service will decrease the follow_up calls to the clients.\nIn the second regression with satisfaction on follow_ups and account for subscription is highly correlated premium subscription levels than elite subscription levels.\n\nPlot the data\n\n\n# Not conditioning on subscription\nsubscription_not_cond &lt;- ggplot(df, aes(x = satisfaction, y = follow_ups)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\n# Conditioning on subscription \nsubscription_cond &lt;- ggplot(df, aes(x = satisfaction, y = follow_ups, color = subscription)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\n# Plot both plots\nsubscription_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsubscription_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\n\n# Load the file\nabtest_online &lt;- readRDS(\"Causal_Data_Science_Data/abtest_online.rds\")\n# Create a Data Frame\ndf1=data.frame(abtest_online)\n\n\ndf&lt;- data.frame(ip = abtest_online$ip, chatbot = abtest_online$chatbot)\nip = abtest_online$ip\ndf$ip &lt;- substr(ip, nchar(ip), nchar(ip))\ndf$chatbot &lt;- as.integer(as.logical(df$chatbot))\ndf$ipn = as.numeric(df$ip)\ndf1&lt;- data.frame(ip = df$ipn, chatbot = df$chatbot)\nX &lt;- tibble(df1)\nX\n\n\n\n  \n\n\n\n\nset.seed(123)  # Set a seed for reproducibility\n\nsss &lt;- c(10, 50, 100, seq(500, 1000, 500))\navg_tbl_ip_lst &lt;- list()\navg_tbl_chatbot_lst &lt;- list()\ntbl_sampled_lst &lt;- list()\n\nfor (ss in sss) {\n  # Sample from the original data\n  df_sampled &lt;- df1 %&gt;% sample_n(ss)\n  \n  # Perform random assignment\n  df_sampled$D &lt;- rbinom(ss, 1, 0.5)\n  \n  # Calculate average characteristics for ip\n  avg_tbl_ip &lt;- df_sampled %&gt;%\n    group_by(D) %&gt;%\n    summarise(mean_ip = mean(ip)) %&gt;%\n    ungroup %&gt;%\n    add_column(sample_size = ss, variable = \"ip\") %&gt;%\n    pivot_wider(names_from = D, names_prefix = \"D_\", values_from = mean_ip) %&gt;%\n    mutate(delta_abs = abs(D_1 - D_0), delta_rel = delta_abs / D_0)\n  \n  avg_tbl_ip_lst[[paste(ss)]] &lt;- avg_tbl_ip\n  \n  # Calculate average characteristics for chatbot\n  avg_tbl_chatbot &lt;- df_sampled %&gt;%\n    group_by(D) %&gt;%\n    summarise(mean_chatbot = mean(chatbot)) %&gt;%\n    ungroup %&gt;%\n    add_column(sample_size = ss, variable = \"chatbot\") %&gt;%\n    pivot_wider(names_from = D, names_prefix = \"D_\", values_from = mean_chatbot) %&gt;%\n    mutate(delta_abs = abs(D_1 - D_0), delta_rel = delta_abs / D_0)\n  \n  avg_tbl_chatbot_lst[[paste(ss)]] &lt;- avg_tbl_chatbot\n}\n\n\n# Combine tables to one larger table\navg_ip &lt;- avg_tbl_ip_lst %&gt;% bind_rows()\navg_chatbot &lt;- avg_tbl_chatbot_lst %&gt;% bind_rows()\navgs_tbl &lt;- avg_ip %&gt;% bind_rows(avg_chatbot)\n\navgs_tbl\n\n\n\n  \n\n\n\n\n# Plot the Average Characteristics\n# Plot convergence\nggplot(avgs_tbl, aes(x = sample_size, y = delta_abs, color = factor(variable))) +\n  geom_line() +\n  facet_wrap(~variable, scales = \"free\") +\n  labs(x = \"Sample size\", y = \"Absolute difference\") +\n  ggtitle(\"Absolute difference of characteristics\\n between groups by sample size\")\n\n\n\n\n\n\n\n\n\nRegression\n\n\ndf2&lt;- data.frame(ip = X$ip, chatbot = X$chatbot, treatment = X$chatbot, outcome = avgs_tbl$delta_abs)\n\n\nlm_ate &lt;- lm(outcome ~ treatment, data = df2)\nsummary(lm_ate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = outcome ~ treatment, data = df2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.37634 -0.36434 -0.20239 -0.06674  1.23042 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 0.369583   0.023212  15.922   &lt;2e-16 ***\n#&gt; treatment   0.006754   0.032697   0.207    0.836    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.517 on 998 degrees of freedom\n#&gt; Multiple R-squared:  4.275e-05,  Adjusted R-squared:  -0.0009592 \n#&gt; F-statistic: 0.04267 on 1 and 998 DF,  p-value: 0.8364\n\n\n\nInclude interaction\n\n\nlm_mod &lt;- lm(outcome ~ treatment * ip + treatment * chatbot, data = df2)\nsummary(lm_mod)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = outcome ~ treatment * ip + treatment * chatbot, \n#&gt;     data = df2)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.39823 -0.35171 -0.20108 -0.05422  1.25493 \n#&gt; \n#&gt; Coefficients: (2 not defined because of singularities)\n#&gt;                     Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)        0.3983482  0.0471358   8.451   &lt;2e-16 ***\n#&gt; treatment         -0.0001204  0.0615057  -0.002    0.998    \n#&gt; ip                -0.0059201  0.0084417  -0.701    0.483    \n#&gt; chatbot                   NA         NA      NA       NA    \n#&gt; treatment:ip       0.0002564  0.0118421   0.022    0.983    \n#&gt; treatment:chatbot         NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.5172 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.001003,   Adjusted R-squared:  -0.002006 \n#&gt; F-statistic: 0.3332 on 3 and 996 DF,  p-value: 0.8014"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\n\n\n# Load the file\nhospdd &lt;- readRDS(\"Causal_Data_Science_Data/hospdd.rds\")\n# Create a Data Frame\ndf=data.frame(hospdd)\n\n1.Compute mean satisfactions\n\n# Convert month to a numeric variable\ndf$month &lt;- as.numeric(as.character(df$month))\n# Create a binary indicator for the post-treatment period\ndf$treatment &lt;- ifelse(df$month &gt;= 4, 1, 0)  # Assuming treatment occurred after month 3\n\n# Calculate mean satisfaction for control hospitals before and after treatment\nmean_satisfaction_control_before &lt;- df %&gt;%\n  filter(procedure == 0, treatment == 0) %&gt;%\n  summarise(mean_satisfaction = sum(satis) / length(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nmean_satisfaction_control_after &lt;- df %&gt;%\n  filter(procedure == 0, treatment == 1) %&gt;%\n  summarise(mean_satisfaction = sum(satis) / length(satis)) %&gt;%\n  pull(mean_satisfaction)\n\n# Calculate mean satisfaction for treated hospitals before and after treatment\nmean_satisfaction_treated_before &lt;- df %&gt;%\n  filter(procedure == 1, treatment == 0) %&gt;%\n  summarise(mean_satisfaction = sum(satis) / length(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nmean_satisfaction_treated_after &lt;- df %&gt;%\n  filter(procedure == 1, treatment == 1) %&gt;%\n  summarise(mean_satisfaction = sum(satis) / length(satis)) %&gt;%\n  pull(mean_satisfaction)\n\n# Print the results\ncat(\"Mean Satisfaction for Control Hospitals Before Treatment:\", mean_satisfaction_control_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals Before Treatment: 3.447765\n\ncat(\"Mean Satisfaction for Control Hospitals After Treatment:\", mean_satisfaction_control_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Control Hospitals After Treatment: 3.38249\n\ncat(\"Mean Satisfaction for Treated Hospitals Before Treatment:\", mean_satisfaction_treated_before, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals Before Treatment: NaN\n\ncat(\"Mean Satisfaction for Treated Hospitals After Treatment:\", mean_satisfaction_treated_after, \"\\n\")\n\n#&gt; Mean Satisfaction for Treated Hospitals After Treatment: 4.363351\n\n\n\nUsing a linear regression to compute and estimate:\n\n\n# 2.1 Linear regression with month + hospital\nlm_satis &lt;- lm(procedure ~ month + hospital, data = df)\nsummary(lm_satis)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = procedure ~ month + hospital, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.56141 -0.23972 -0.04324  0.24191  0.68376 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.2501884  0.0088703   28.20   &lt;2e-16 ***\n#&gt; month        0.0869378  0.0016108   53.97   &lt;2e-16 ***\n#&gt; hospital    -0.0156497  0.0002513  -62.26   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.2928 on 7365 degrees of freedom\n#&gt; Multiple R-squared:  0.4797, Adjusted R-squared:  0.4795 \n#&gt; F-statistic:  3395 on 2 and 7365 DF,  p-value: &lt; 2.2e-16\n\n\n\n# 2.2 Linear regression as.factor(month) + as.factor(hospital)\nlm_satis &lt;- lm(procedure ~ as.factor(month) + as.factor(hospital), data = df)\nsummary(lm_satis)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = procedure ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.2921 -0.2079  0.0000  0.2079  0.2921 \n#&gt; \n#&gt; Coefficients:\n#&gt;                         Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            2.921e-01  1.890e-02   15.45   &lt;2e-16 ***\n#&gt; as.factor(month)2     -5.435e-14  9.981e-03    0.00        1    \n#&gt; as.factor(month)3     -5.076e-15  9.981e-03    0.00        1    \n#&gt; as.factor(month)4      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(month)5      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(month)6      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(month)7      4.159e-01  9.981e-03   41.67   &lt;2e-16 ***\n#&gt; as.factor(hospital)2   2.334e-14  2.639e-02    0.00        1    \n#&gt; as.factor(hospital)3   2.082e-15  2.711e-02    0.00        1    \n#&gt; as.factor(hospital)4   1.161e-14  2.526e-02    0.00        1    \n#&gt; as.factor(hospital)5   1.641e-14  2.526e-02    0.00        1    \n#&gt; as.factor(hospital)6   1.329e-14  2.526e-02    0.00        1    \n#&gt; as.factor(hospital)7  -3.935e-15  2.441e-02    0.00        1    \n#&gt; as.factor(hospital)8   2.745e-14  2.608e-02    0.00        1    \n#&gt; as.factor(hospital)9  -3.100e-14  2.673e-02    0.00        1    \n#&gt; as.factor(hospital)10  1.275e-14  2.639e-02    0.00        1    \n#&gt; as.factor(hospital)11 -9.250e-15  2.608e-02    0.00        1    \n#&gt; as.factor(hospital)12  3.702e-14  2.673e-02    0.00        1    \n#&gt; as.factor(hospital)13 -1.529e-14  2.578e-02    0.00        1    \n#&gt; as.factor(hospital)14 -4.284e-14  2.711e-02    0.00        1    \n#&gt; as.factor(hospital)15 -2.265e-14  2.711e-02    0.00        1    \n#&gt; as.factor(hospital)16  2.440e-14  2.639e-02    0.00        1    \n#&gt; as.factor(hospital)17  2.345e-14  2.752e-02    0.00        1    \n#&gt; as.factor(hospital)18 -1.045e-14  3.205e-02    0.00        1    \n#&gt; as.factor(hospital)19 -5.000e-01  2.711e-02  -18.45   &lt;2e-16 ***\n#&gt; as.factor(hospital)20 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)21 -5.000e-01  2.797e-02  -17.88   &lt;2e-16 ***\n#&gt; as.factor(hospital)22 -5.000e-01  2.752e-02  -18.17   &lt;2e-16 ***\n#&gt; as.factor(hospital)23 -5.000e-01  2.711e-02  -18.45   &lt;2e-16 ***\n#&gt; as.factor(hospital)24 -5.000e-01  2.902e-02  -17.23   &lt;2e-16 ***\n#&gt; as.factor(hospital)25 -5.000e-01  3.114e-02  -16.06   &lt;2e-16 ***\n#&gt; as.factor(hospital)26 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)27 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)28 -5.000e-01  2.797e-02  -17.88   &lt;2e-16 ***\n#&gt; as.factor(hospital)29 -5.000e-01  2.673e-02  -18.70   &lt;2e-16 ***\n#&gt; as.factor(hospital)30 -5.000e-01  3.205e-02  -15.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)31 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)32 -5.000e-01  2.673e-02  -18.70   &lt;2e-16 ***\n#&gt; as.factor(hospital)33 -5.000e-01  2.639e-02  -18.95   &lt;2e-16 ***\n#&gt; as.factor(hospital)34 -5.000e-01  2.481e-02  -20.15   &lt;2e-16 ***\n#&gt; as.factor(hospital)35 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)36 -5.000e-01  2.578e-02  -19.39   &lt;2e-16 ***\n#&gt; as.factor(hospital)37 -5.000e-01  3.114e-02  -16.06   &lt;2e-16 ***\n#&gt; as.factor(hospital)38 -5.000e-01  2.608e-02  -19.18   &lt;2e-16 ***\n#&gt; as.factor(hospital)39 -5.000e-01  2.752e-02  -18.17   &lt;2e-16 ***\n#&gt; as.factor(hospital)40 -5.000e-01  2.608e-02  -19.18   &lt;2e-16 ***\n#&gt; as.factor(hospital)41 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)42 -5.000e-01  2.846e-02  -17.57   &lt;2e-16 ***\n#&gt; as.factor(hospital)43 -5.000e-01  2.711e-02  -18.45   &lt;2e-16 ***\n#&gt; as.factor(hospital)44 -5.000e-01  3.034e-02  -16.48   &lt;2e-16 ***\n#&gt; as.factor(hospital)45 -5.000e-01  2.551e-02  -19.60   &lt;2e-16 ***\n#&gt; as.factor(hospital)46 -5.000e-01  2.752e-02  -18.17   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.2473 on 7316 degrees of freedom\n#&gt; Multiple R-squared:  0.6313, Adjusted R-squared:  0.6287 \n#&gt; F-statistic: 245.6 on 51 and 7316 DF,  p-value: &lt; 2.2e-16\n\n\nThe difference is that the factor function gives detailed values of the estimates for all the months and all the hospitals whereas the normal regression is only confined to two coefficients."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "1 Assignment I\n\n# Given probabilities\nP_T &lt;- 0.8  # Probability of being on time\nP_Tbar &lt;- 0.2  # Probability of not being on time\nP_S &lt;- 0.3  # Probability of a change in scope\nP_Sbar &lt;- 0.7  # Probability of no change in scope\n\n# Calculating probabilities\nP_T_and_S &lt;- P_T * P_S\nP_T_and_Sbar &lt;- P_T * P_Sbar\nP_Tbar_and_S &lt;- P_Tbar * P_S\nP_Tbar_and_Sbar &lt;- P_Tbar * P_Sbar\n\n# Sum of probabilities\nsum_of_probabilities &lt;- P_T_and_S + P_T_and_Sbar + P_Tbar_and_S + P_Tbar_and_Sbar\n\n# Displaying the results\ncat(\"P(T ∩ S):\", P_T_and_S, \"\\n\")\n\n#&gt; P(T ∩ S): 0.24\n\ncat(\"P(T ∩ S'): \", P_T_and_Sbar, \"\\n\")\n\n#&gt; P(T ∩ S'):  0.56\n\ncat(\"P(T' ∩ S):\", P_Tbar_and_S, \"\\n\")\n\n#&gt; P(T' ∩ S): 0.06\n\ncat(\"P(T' ∩ S'): \", P_Tbar_and_Sbar, \"\\n\")\n\n#&gt; P(T' ∩ S'):  0.14\n\ncat(\"Sum of probabilities:\", sum_of_probabilities, \"\\n\")\n\n#&gt; Sum of probabilities: 1\n\n\n\n\n2 Assignment II\n\n# Given probabilities\nP_A &lt;- 0.423\nP_B &lt;- 0.278\nP_C &lt;- 0.1\nP_AB &lt;- 0.073\nP_BC &lt;- 0.033\nP_AC &lt;- 0.088\nP_ABC &lt;- 0.005\n\n# Percentage of customers using all three devices\nP_all_three &lt;- P_ABC * 100\nP_all_three\n\n#&gt; [1] 0.5\n\n# Calculate the percentage of customers using at least two devices\nP_at_least_two &lt;- (P_A + P_B + P_C - P_AB - P_BC - P_AC + P_ABC) * 100\nP_at_least_two\n\n#&gt; [1] 61.2\n\n# Calculate the percentage of customers using only one device\nP_only_one &lt;- (P_A - P_AB - P_AC + P_ABC + P_B - P_AB - P_BC + P_ABC + P_C - P_AC -P_BC + P_ABC) * 100\nP_only_one\n\n#&gt; [1] 42.8\n\n\n\n\n3 Assignment III\n\n# Given probabilities\nP_B_A &lt;- 0.97\nP_B_notA &lt;- 0.01\nP_A &lt;- 0.04\nnot_P_A &lt;- 1 - P_A\n\n# Calculate the probability B\nP_B &lt;- P_B_A * P_A + P_B_notA* not_P_A\nP_B\n\n#&gt; [1] 0.0484\n\n# Calculate the probability A|B\nP_A_B &lt;- P_B_A * P_A /P_B\nP_A_B\n\n#&gt; [1] 0.8016529\n\n# Calculate the probability not_A|B\nP_not_A_B &lt;- P_B_notA * not_P_A / P_B\nP_not_A_B\n\n#&gt; [1] 0.1983471\n\n\nThe following sentence: These results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.16% that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "5.1 Header 2",
    "text": "5.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "1 Assignmnet\n\n# Load the Library\nlibrary(tidyverse)\n\n\n# Load the file\ncar_prices &lt;- readRDS(\"Causal_Data_Science_Data/car_prices.rds\")\n# Create a Data Frame\ndf=data.frame(car_prices)\n\n\nCheck the dimensions\n\n\ndim(df)\n\n#&gt; [1] 181  22\n\n\nThere are 181 rows and 22 columns in the data file.\n\nUsing appropriate commands to get a more detailed look at the data.\n\n\nhead(df)\n\n\n\n  \n\n\n\n\nglimpse(df)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n\n\nsummary(df)\n\n#&gt;   aspiration         doornumber          carbody           drivewheel       \n#&gt;  Length:181         Length:181         Length:181         Length:181        \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;  enginelocation       wheelbase        carlength        carwidth    \n#&gt;  Length:181         Min.   : 86.60   Min.   :141.1   Min.   :60.30  \n#&gt;  Class :character   1st Qu.: 94.50   1st Qu.:166.3   1st Qu.:64.00  \n#&gt;  Mode  :character   Median : 96.50   Median :173.0   Median :65.40  \n#&gt;                     Mean   : 98.21   Mean   :173.3   Mean   :65.74  \n#&gt;                     3rd Qu.:100.40   3rd Qu.:180.2   3rd Qu.:66.50  \n#&gt;                     Max.   :120.90   Max.   :208.1   Max.   :72.30  \n#&gt;    carheight       curbweight    enginetype        cylindernumber    \n#&gt;  Min.   :47.80   Min.   :1488   Length:181         Length:181        \n#&gt;  1st Qu.:52.00   1st Qu.:2122   Class :character   Class :character  \n#&gt;  Median :53.70   Median :2410   Mode  :character   Mode  :character  \n#&gt;  Mean   :53.58   Mean   :2521                                        \n#&gt;  3rd Qu.:55.50   3rd Qu.:2910                                        \n#&gt;  Max.   :59.80   Max.   :4066                                        \n#&gt;    enginesize     fuelsystem          boreratio         stroke    \n#&gt;  Min.   : 61.0   Length:181         Min.   :2.540   Min.   :2.07  \n#&gt;  1st Qu.: 98.0   Class :character   1st Qu.:3.150   1st Qu.:3.08  \n#&gt;  Median :120.0   Mode  :character   Median :3.310   Median :3.23  \n#&gt;  Mean   :127.1                      Mean   :3.325   Mean   :3.23  \n#&gt;  3rd Qu.:141.0                      3rd Qu.:3.590   3rd Qu.:3.40  \n#&gt;  Max.   :326.0                      Max.   :3.940   Max.   :4.17  \n#&gt;  compressionratio   horsepower       peakrpm        citympg     \n#&gt;  Min.   : 7.000   Min.   : 48.0   Min.   :4200   Min.   :13.00  \n#&gt;  1st Qu.: 8.500   1st Qu.: 70.0   1st Qu.:4800   1st Qu.:19.00  \n#&gt;  Median : 9.000   Median : 95.0   Median :5200   Median :24.00  \n#&gt;  Mean   : 8.848   Mean   :106.2   Mean   :5182   Mean   :24.85  \n#&gt;  3rd Qu.: 9.400   3rd Qu.:116.0   3rd Qu.:5500   3rd Qu.:30.00  \n#&gt;  Max.   :11.500   Max.   :288.0   Max.   :6600   Max.   :49.00  \n#&gt;    highwaympg        price      \n#&gt;  Min.   :16.00   Min.   : 5118  \n#&gt;  1st Qu.:25.00   1st Qu.: 7609  \n#&gt;  Median :30.00   Median : 9980  \n#&gt;  Mean   :30.48   Mean   :12999  \n#&gt;  3rd Qu.:34.00   3rd Qu.:16430  \n#&gt;  Max.   :54.00   Max.   :45400\n\n\nThe data types present are character and numeric data type. The numbers differ from string regarding the type of information represented. Numeric data is used for quantitative measurements, while character data is used for textual information.\n\nLinear regression\n\n\nlm_all &lt;- lm(price ~ ., data = df)\nsummary(lm_all)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nAs from the summary of linear regression, All factors estimate with a p-value less than 0.05(arbitrary significance level-alpha) are relevant for the pricing of a car like carbodyhatchback, carbodywagon, enginelocationrear, carwidth, enginetypeohc, enginetypeohcv, cylindernumberfive, cylindernumberfour, cylindernumbersix, cylindernumbertwelve, enginesize, stroke and peakrpm.\n\nChoosing one regressor\n\n\nlm_imp &lt;- lm(price ~ carwidth, data = df)\nsummary(lm_imp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ carwidth, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11461.9  -2736.1   -870.9    589.5  26156.7 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -174994.0    12663.2  -13.82   &lt;2e-16 ***\n#&gt; carwidth       2859.5      192.5   14.85   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5415 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.5521, Adjusted R-squared:  0.5496 \n#&gt; F-statistic: 220.6 on 1 and 179 DF,  p-value: &lt; 2.2e-16\n\n\n\n# 1. Data type and what value it can take on\nstr(car_prices$carwidth)\n\n#&gt;  num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n\n# 2. Effect it has on the price and what changing the value would have as a result\nsummary(lm_all)$coefficients['carwidth',]\n\n#&gt;     Estimate   Std. Error      t value     Pr(&gt;|t|) \n#&gt; 7.318188e+02 2.445333e+02 2.992716e+00 3.258264e-03\n\n# 3. Whether its effect is statistically significant\nsummary(lm_all)$coefficients['carwidth','Pr(&gt;|t|)']\n\n#&gt; [1] 0.003258264\n\n\n4.1 The regressor used was car width and it belongs to the numeric variables (discrete/continuous). It can take numeric values.\n4.2 As our estimate is positive (2859.5), we have a positive effect. As a result increasing the car width would increase the price of the car.\n4.3 Yes, it is statistically significant as the p value is lower than our significance level (0.05)\n\nAdd a variable seat_heating to the data\n\n\ndf_new &lt;- df %&gt;% mutate(seatheating = TRUE)\ndf_new\n\n\n\n  \n\n\n\n\n# New regression\nlm_new &lt;- lm(price ~ seatheating , data = df_new)\nsummary(lm_new)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seatheating, data = df_new)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -7881  -5390  -3019   3431  32401 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)      12999.4      599.7   21.68   &lt;2e-16 ***\n#&gt; seatheatingTRUE       NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8068 on 180 degrees of freedom\n\n\nThere is no coefficient for this regression as there is no relation between seat heating and pricing."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(dagitty)\n\n\n# Load the file\ncoupon&lt;- readRDS(\"Causal_Data_Science_Data/coupon.rds\")\n# Create a Data Frame\ndf=data.frame(coupon)\n\n\n# Define cut-off\nc0 = 60\n# Half bandwidth\nbw &lt;- c0 + c(-2.5, 2.5)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df %&gt;% filter(days_since_last %&gt;% between(bw[1], c0-1e-4))\ndf_bw_above &lt;- df %&gt;% filter(days_since_last %&gt;% between(c0, bw[2]))\n\n# Alternative way to define tables\n# df_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1], days_since_last  &lt; c0)\n# df_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0, days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n#&gt; [1] 181   4\n\n\n\n# Define cut-off\nc0 = 60\n# Double bandwidth\nbw &lt;- c0 + c(-10, 10)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df %&gt;% filter(days_since_last %&gt;% between(bw[1], c0-1e-4))\ndf_bw_above &lt;- df %&gt;% filter(days_since_last %&gt;% between(c0, bw[2]))\n\n# Alternative way to define tables\n# df_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1], days_since_last  &lt; c0)\n# df_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0, days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n#&gt; [1] 629   4\n\n\n\n# Load the file\nshipping&lt;- readRDS(\"Causal_Data_Science_Data/shipping.rds\")\n# Create a Data Frame\ndf2=data.frame(shipping)\n\n\n# Density test\n# Check for continuous density along running variable. Manipulations could \n# lead to running variable being \"crowded\" right after cutoff.\nlibrary(rddensity)\nrddd &lt;- rddensity(df2$purchase_amount, c = 30)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\n\n\n# Visually check continuity at running variable\nrdd_plot &lt;- rdplotdensity(rddd, df2$purchase_amount, plotN = 100)\n\n\n\n\n\n\n\n\nThe plot confirms as they did not overlap, we would have to suspect some kind of manipulation around the cut-off and could not use RDD to obtain valid results."
  }
]